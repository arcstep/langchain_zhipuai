{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydantic==1.10.13\n",
    "# !pip install pydantic==2.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_zhipu import ChatZhipuAI\n",
    "llm = ChatZhipuAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 或者你想自己临时指定API_KEY\n",
    "import os\n",
    "llm = ChatZhipuAI(api_key=os.getenv('ZHIPUAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我是一名人工智能助手，基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型 GLM-130B 开发而成。我的任务是针对用户的问题和要求提供适当的答复和支持。', response_metadata={'id': '8513537775423542980', 'created': 1711533201, 'token_usage': {'prompt_tokens': 9, 'completion_tokens': 49, 'total_tokens': 58}, 'model_name': 'glm-4', 'finish_reason': 'stop'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"你是什么模型？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我|是一名|人工智能|助手|，|基于|清华大学| K|EG| 实|验|室|和|智|谱| AI| 公司|于| |202|3| 年|共同|训练|的语言|模型| GL|M|2| 开|发|而成|。|我的|任务是|针对|用户|的问题|和要求|提供|适当的|答复|和支持|。||"
     ]
    }
   ],
   "source": [
    "for s in llm.stream(\"你是什么模型？\"):\n",
    "  print(s.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我|是一名|人工智能|助手|，|基于|清华大学| K|EG| 实|验|室|和|智|谱| AI| 公司|于| |202|3| 年|共同|训练|的语言|模型| GL|M|2| 开|发|而成|。|我的|任务是|针对|用户|的问题|和要求|提供|适当的|答复|和支持|。||"
     ]
    }
   ],
   "source": [
    "async for s in llm.astream(\"你是什么模型？\"):\n",
    "  print(s.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='我使用的模型是清华大学 KEG 实验室和智谱AI共同训练的 GLM 模型，一种基于 Transformer 的通用预训练语言模型。Transformer 模型是一种基于自注意力机制的深度神经网络模型，经常用于处理序列数据。\\n\\n我可能用到最大的模型是 GLM-130B，具有 1300 亿参数，支持中英双语。我具体使用的模型规模视应用场景可能会有所变化。', response_metadata={'id': '8516977013436319484', 'created': 1711628383, 'token_usage': {'prompt_tokens': 9, 'completion_tokens': 91, 'total_tokens': 100}, 'model_name': 'glm-4', 'finish_reason': 'stop'}),\n",
       " AIMessage(content='我是一个人工智能助手模型，基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型 GLM2 开发而成。我的任务是针对用户的问题和要求提供适当的答复和支持。', response_metadata={'id': '8516978559624613918', 'created': 1711628381, 'token_usage': {'prompt_tokens': 9, 'completion_tokens': 47, 'total_tokens': 56}, 'model_name': 'glm-4', 'finish_reason': 'stop'}),\n",
       " AIMessage(content='我是一个基于语言的人工智能模型，名为 ChatGLM。我是由清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型 GLM-130B 开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。', response_metadata={'id': '8516984813097069119', 'created': 1711628382, 'token_usage': {'prompt_tokens': 9, 'completion_tokens': 56, 'total_tokens': 65}, 'model_name': 'glm-4', 'finish_reason': 'stop'}),\n",
       " AIMessage(content='我是一个基于语言的人工智能模型，名为 ChatGLM，现在又名智谱清言。我是由清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型 GLM-130B 开发的，我的任务是针对用户的问题和要求提供适当的答复和支持。我可以在各种场景中应用，如对话助手、知识图谱问答、文本生成等。', response_metadata={'id': '8516978250386934515', 'created': 1711628383, 'token_usage': {'prompt_tokens': 9, 'completion_tokens': 82, 'total_tokens': 91}, 'model_name': 'glm-4', 'finish_reason': 'stop'}),\n",
       " AIMessage(content='我是一名人工智能助手，基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型 GLM2 开发而成。我的任务是针对用户的问题和要求提供适当的答复和支持。', response_metadata={'id': '8516983232549062124', 'created': 1711628382, 'token_usage': {'prompt_tokens': 9, 'completion_tokens': 47, 'total_tokens': 56}, 'model_name': 'glm-4', 'finish_reason': 'stop'}),\n",
       " AIMessage(content='我使用的模型是清华大学 KEG 实验室和智谱AI共同训练的 GLM 模型，一种基于 Transformer 的通用预训练语言模型。Transformer 模型是一种基于自注意力机制的深度神经网络模型，经常用于处理序列数据。\\n\\n我可能用到最大的模型是 GLM-130B，具有 1300 亿参数，支持中英双语。我具体使用的模型规模视应用场景可能会有所变化。', response_metadata={'id': '8516976979076573621', 'created': 1711628383, 'token_usage': {'prompt_tokens': 9, 'completion_tokens': 91, 'total_tokens': 100}, 'model_name': 'glm-4', 'finish_reason': 'stop'}),\n",
       " AIMessage(content='我是一名人工智能助手，基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型 GLM-130B 开发而成。我的任务是针对用户的问题和要求提供适当的答复和支持。', response_metadata={'id': '8516983129469829566', 'created': 1711628382, 'token_usage': {'prompt_tokens': 9, 'completion_tokens': 49, 'total_tokens': 58}, 'model_name': 'glm-4', 'finish_reason': 'stop'}),\n",
       " AIMessage(content='我使用的模型是清华大学 KEG 实验室和智谱AI共同训练的 GLM 模型，一种基于 Transformer 的通用预训练语言模型。Transformer 模型是一种基于自注意力机制的深度神经网络模型，经常用于处理序列数据。\\n\\n我可能用到最大的模型是 GLM-130B，具有 1300 亿参数，支持中英双语。我具体使用的模型规模视应用场景可能会有所变化。', response_metadata={'id': '8516976979076573620', 'created': 1711628383, 'token_usage': {'prompt_tokens': 9, 'completion_tokens': 91, 'total_tokens': 100}, 'model_name': 'glm-4', 'finish_reason': 'stop'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.batch([\n",
    "    \"你是什么模型？\",\n",
    "    \"你是什么模型？\",\n",
    "    \"你是什么模型？\",\n",
    "    \"你是什么模型？\",\n",
    "    \"你是什么模型？\",\n",
    "    \"你是什么模型？\",\n",
    "    \"你是什么模型？\",\n",
    "    \"你是什么模型？\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools-Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_8513532209145851088', 'function': {'arguments': '{\"query\":\"langchain_chinese是啥？请查询本地资料回答。\"}', 'name': 'search'}, 'type': 'function'}]}, response_metadata={'id': '8513532209145851088', 'created': 1711547168, 'token_usage': {'prompt_tokens': 133, 'completion_tokens': 25, 'total_tokens': 158}, 'model_name': 'glm-4', 'finish_reason': 'tool_calls'})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"查询 langchan 资料; args: query 类型为字符串，描述用户的问题.\"\"\"\n",
    "    return \"langchain_chinese 是一个为中国大模型优化的langchain模块\"\n",
    "\n",
    "llm.bind(tools=[convert_to_openai_tool(search)]).invoke(\"langchain_chinese是啥？请查询本地资料回答。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## web_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='从您提供的参考信息中，可以看出《银河写手》是一部受到多位电影人如周星驰和姜文高度评价的电影，它在金鸡影展上获得了观众的积极反馈，被认为是一部黑马电影。这部电影由李阔和单丹丹执导，讲述了影视行业的故事，融合了虚幻与现实，是对上世纪90年代北京青春的一种记录。它不仅参考了多部经典影片，还融入了编剧的真实生活经历，试图呈现复古胶片的质感。\\n\\n因此，如果喜欢深入探讨影视行业、具有文艺气质并且情感真挚的电影，那么《银河写手》可能会是一个不错的选择。\\n\\n当然，电影的选择也取决于个人的喜好。如果您偏爱喜剧，可能会对周星驰的其他作品如《长江七号》或《美人鱼》感兴趣，这些电影展现了周星驰独特的无厘头风格和他作为导演发掘演员的能力。\\n\\n总之，选择哪部电影“好看”主要还是根据您个人的兴趣和喜好来定。不同的电影有不同的魅力和价值，建议根据电影的类型、题材、导演和演员阵容等因素来做出选择。', response_metadata={'id': '8513531659390035186', 'created': 1711532324, 'token_usage': {'prompt_tokens': 616, 'completion_tokens': 220, 'total_tokens': 836}, 'model_name': 'glm-4', 'finish_reason': 'stop'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_zhipu import convert_to_web_search_tool\n",
    "llm.bind(tools=[convert_to_web_search_tool(search_query=\"周星驰电影\")]).invoke(\"哪部电影好看？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='马冬梅住在楼上322房间。', response_metadata={'id': '8513532209145848833', 'created': 1711546897, 'token_usage': {'prompt_tokens': 82, 'completion_tokens': 11, 'total_tokens': 93}, 'model_name': 'glm-4', 'finish_reason': 'stop'})"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_zhipu import convert_to_retrieval_tool\n",
    "llm.bind(tools=[convert_to_retrieval_tool(knowledge_id=\"1772979648448397312\")]).invoke(\"你知道马冬梅住哪里吗？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatZhipuAI(client=<langchain_zhipu._client.ZhipuAI object at 0x10c437220>), kwargs={'tools': [{'type': 'function', 'function': {'name': 'search', 'description': 'search(query: str) -> str - 查询 langchan 资料; args: query 类型为字符串，描述用户的问题.', 'parameters': {'type': 'object', 'properties': {'query': {'type': 'string'}}, 'required': ['query']}}}]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.bind(tools=[convert_to_openai_tool(search)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用官方接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_zhipu import ZhipuAI\n",
    "\n",
    "client = ZhipuAI()\n",
    "\n",
    "tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"search\",\n",
    "                \"description\": \"search(query: str) -> str - 查询 langchan 资料; args: query 类型为字符串，描述用户的问题.'\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"query\": {\n",
    "                            \"type\": \"string\"\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"query\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Completion(model='glm-4', created=1711538632, choices=[CompletionChoice(index=0, finish_reason='tool_calls', message=CompletionMessage(content=None, role='assistant', tool_calls=[CompletionMessageToolCall(id='call_8513538359539114557', function=Function(arguments='{\"query\":\"langchain是什么？\"}', name='search'), type='function')]))], request_id='8513538359539114557', id='8513538359539114557', usage=CompletionUsage(prompt_tokens=128, completion_tokens=16, total_tokens=144))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.chat.completions.create(\n",
    "    model=\"glm-4\",\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"langchain是什么？请帮我查询\"\n",
    "        }\n",
    "    ],\n",
    "    tools = tools,\n",
    "    tool_choice=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIInternalError",
     "evalue": "Error code: 500, with error text {\"error\":{\"code\":\"500\",\"message\":\"Internal Error\"}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIInternalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mglm-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlangchain是什么？\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/langchain_zhipuai/langchain_zhipu/api_resource/chat/completions.py:48\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, model, request_id, do_sample, stream, temperature, top_p, max_tokens, seed, messages, stop, sensitive_word_check, tools, tool_choice, extra_headers, disable_strict_validation, timeout)\u001b[0m\n\u001b[1;32m     46\u001b[0m     _cast_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m     47\u001b[0m     _stream_cls \u001b[38;5;241m=\u001b[39m StreamResponse[\u001b[38;5;28mobject\u001b[39m]\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrequest_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdo_sample\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msensitive_word_check\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msensitive_word_check\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_user_request_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_cast_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_stream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/langchain_zhipuai/langchain_zhipu/core/_http_client.py:290\u001b[0m, in \u001b[0;36mHttpClient.post\u001b[0;34m(self, path, body, cast_type, options, files, enable_stream, stream_cls)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    278\u001b[0m         path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m         stream_cls: \u001b[38;5;28mtype\u001b[39m[StreamResponse[Any]] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    286\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m StreamResponse:\n\u001b[1;32m    287\u001b[0m     opts \u001b[38;5;241m=\u001b[39m ClientRequestParam\u001b[38;5;241m.\u001b[39mconstruct(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mmake_httpx_files(files), url\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m    288\u001b[0m                                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_stream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_stream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/langchain_zhipuai/langchain_zhipu/core/_http_client.py:249\u001b[0m, in \u001b[0;36mHttpClient.request\u001b[0;34m(self, cast_type, params, enable_stream, stream_cls)\u001b[0m\n\u001b[1;32m    247\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# raise err\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "\u001b[0;31mAPIInternalError\u001b[0m: Error code: 500, with error text {\"error\":{\"code\":\"500\",\"message\":\"Internal Error\"}}"
     ]
    }
   ],
   "source": [
    "client.chat.completions.create(\n",
    "    model=\"glm-4\",\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"langchain是什么？\"\n",
    "        }\n",
    "    ],\n",
    "    tools = tools,\n",
    "    tool_choice=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "{'actions': [OpenAIToolAgentAction(tool='search', tool_input={'query': 'langchain_chinese是什么'}, log=\"\\nInvoking: `search` with `{'query': 'langchain_chinese是什么'}`\\nresponded: 我需要使用搜索引擎来获取相关信息。<|assistant|>\\n\\n\", message_log=[AIMessageChunk(content='我需要使用搜索引擎来获取相关信息。<|assistant|>', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_8513530731677143587', 'function': {'arguments': '{\"query\":\"langchain_chinese是什么\"}', 'name': 'search'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_callstool_calls'})], tool_call_id='call_8513530731677143587')], 'messages': [AIMessageChunk(content='我需要使用搜索引擎来获取相关信息。<|assistant|>', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_8513530731677143587', 'function': {'arguments': '{\"query\":\"langchain_chinese是什么\"}', 'name': 'search'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_callstool_calls'})]}\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search` with `{'query': 'langchain_chinese是什么'}`\n",
      "responded: 我需要使用搜索引擎来获取相关信息。<|assistant|>\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mlangchain_chinese 是一个为中国大模型优化的langchain模块\u001b[0m{'steps': [AgentStep(action=OpenAIToolAgentAction(tool='search', tool_input={'query': 'langchain_chinese是什么'}, log=\"\\nInvoking: `search` with `{'query': 'langchain_chinese是什么'}`\\nresponded: 我需要使用搜索引擎来获取相关信息。<|assistant|>\\n\\n\", message_log=[AIMessageChunk(content='我需要使用搜索引擎来获取相关信息。<|assistant|>', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_8513530731677143587', 'function': {'arguments': '{\"query\":\"langchain_chinese是什么\"}', 'name': 'search'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_callstool_calls'})], tool_call_id='call_8513530731677143587'), observation='langchain_chinese 是一个为中国大模型优化的langchain模块')], 'messages': [FunctionMessage(content='langchain_chinese 是一个为中国大模型优化的langchain模块', name='search')]}\n",
      "\u001b[32;1m\u001b[1;3m根据我的搜索结果，langchain_chinese 是一个为中国大模型优化的 langchain 模块。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'output': '根据我的搜索结果，langchain_chinese 是一个为中国大模型优化的 langchain 模块。', 'messages': [AIMessage(content='根据我的搜索结果，langchain_chinese 是一个为中国大模型优化的 langchain 模块。')]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor, Tool, create_openai_tools_agent\n",
    "from langchain import hub\n",
    "\n",
    "tools = [search]\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "for chunk in agent_executor.stream({\"input\": \"langchain_chinese是什么？\"}):\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM-4V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='图中有一片蓝天白云，海面，岩石，以及右下角的树木。具体地，在图片右边有深绿色的树木，树的右边是灰黑色的岩石滩，延伸到海中。海的另一侧有远方的陆地，陆地上似乎有一些建筑。天空是蓝色的，上面飘着一些白色的云朵。')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_zhipu import ChatZhipuAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm4v = ChatZhipuAI(model=\"glm-4v\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"图里有什么\"\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\" : \"https://img1.baidu.com/it/u=1369931113,3388870256&fm=253&app=138&size=w931&n=0&f=JPEG&fmt=auto?sec=1703696400&t=f3028c7a1dca43a080aeb8239f09cc2f\"\n",
    "            }\n",
    "          }\n",
    "        ]),\n",
    "])\n",
    "\n",
    "(prompt|llm4v).invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 91, 882, 91, 29]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_zhipu import ChatZhipuAI\n",
    "\n",
    "llm = ChatZhipuAI(allowed_special=set())\n",
    "llm.get_token_ids('<|user|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(text=\"你好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_zhipyu_kernel",
   "language": "python",
   "name": "langchain_zhipyu_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
